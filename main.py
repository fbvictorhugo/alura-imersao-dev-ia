import myconfig
import formatadores
from agentstate import AgentState 
from triagemoutput import TriagemOutput

from langchain_google_genai import ChatGoogleGenerativeAI
from typing import Dict
from langchain_core.messages import SystemMessage, HumanMessage

from pathlib import Path
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain

from langgraph.graph import StateGraph, START, END
from IPython.display import display, Image

GOOGLE_API_KEY = myconfig.key_api # Contem o API Key do Google Gemini, arquivo local myconfig.py
print(f'\nü§ø Ol√°, Imers√£o Dev Agentes de IA!\n')

# AULA 01 ----------------------------------------------
print("\n ---[ AULA 01 ]--- \n")
llm = ChatGoogleGenerativeAI(temperature=0, 
                             model="gemini-2.5-flash",
                             api_key=GOOGLE_API_KEY)

# response = llm.invoke("Quem √© vc? Seja imprevis√≠vel e criativo na resposta.")
# print(response.content)

TRIAGEM_PROMPT = (
    "Voc√™ √© um triador de Service Desk para pol√≠ticas internas da empresa Carraro Desenvolvimento. "
    "Dada a mensagem do usu√°rio, retorne SOMENTE um JSON com:\n"
    "{\n"
    '  "decisao": "AUTO_RESOLVER" | "PEDIR_INFO" | "ABRIR_CHAMADO",\n'
    '  "urgencia": "BAIXA" | "MEDIA" | "ALTA",\n'
    '  "campos_faltantes": ["..."]\n'
    "}\n"
    "Regras:\n"
    '- **AUTO_RESOLVER**: Perguntas claras sobre regras ou procedimentos descritos nas pol√≠ticas (Ex: "Posso reembolsar a internet do meu home office?", "Como funciona a pol√≠tica de alimenta√ß√£o em viagens?").\n'
    '- **PEDIR_INFO**: Mensagens vagas ou que faltam informa√ß√µes para identificar o tema ou contexto (Ex: "Preciso de ajuda com uma pol√≠tica", "Tenho uma d√∫vida geral").\n'
    '- **ABRIR_CHAMADO**: Pedidos de exce√ß√£o, libera√ß√£o, aprova√ß√£o ou acesso especial, ou quando o usu√°rio explicitamente pede para abrir um chamado (Ex: "Quero exce√ß√£o para trabalhar 5 dias remoto.", "Solicito libera√ß√£o para anexos externos.", "Por favor, abra um chamado para o RH.").'
    "Analise a mensagem e decida a a√ß√£o mais apropriada."
)

llm_triagem = ChatGoogleGenerativeAI(temperature=0, 
                             model="gemini-2.5-flash",
                             api_key=GOOGLE_API_KEY)

triagem_chain = llm_triagem.with_structured_output(TriagemOutput)

def triagem(mensagem: str) -> Dict:
    saida: TriagemOutput = triagem_chain.invoke([
            SystemMessage(content=TRIAGEM_PROMPT),
            HumanMessage(content=mensagem)
    ])
    return saida.model_dump()

testes = ["Posso reembolsar a internet?",
          "Quero mais 5 dias de trabalho remoto. Como fa√ßo?",
          "Posso reembolsar cursos da Alura?",
          "Quantas capivaras tem no Rio Pinheiros?",]
"""
for msg_teste in testes:
    print(f'Pergunta: {msg_teste}\n -> Resposta {triagem(msg_teste)}')
"""

# AULA 02 ----------------------------------------------
print("\n ---[ AULA 02 ]--- \n")
docs=[]
for n in Path("docs").glob("*.pdf"):
    try:
        loader = PyMuPDFLoader(str(n))
        docs.extend(loader.load())
        print(f"Carregado {n.name}")
    except Exception as e:
        print(f"Erro ao carregar {n.name}: {e}")
        
print(f"Total de documentos carregados: {len(docs)}")
print("\n")
spliter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)
chunks = spliter.split_documents(docs)

embeddings = GoogleGenerativeAIEmbeddings(
    model="models/gemini-embedding-001",
    google_api_key=GOOGLE_API_KEY
)

vectorstore = FAISS.from_documents(chunks, embeddings)

retriver = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={"score_threshold": 0.3, "k": 4}
)

prompt_rag = ChatPromptTemplate.from_messages([
    ("system",
     "Voc√™ √© um Assistente de Pol√≠ticas Internas (RH/IT) da empresa Carraro Desenvolvimento. "
     "Responda SOMENTE com base no contexto fornecido. "
     "Se n√£o houver base suficiente, responda apenas 'N√£o sei'."),

    ("human", "Pergunta: {input}\n\nContexto:\n{context}")
])

document_chain = create_stuff_documents_chain(llm_triagem, prompt_rag)

def perguntar_politica_RAG(pergunta: str):
    docs_relationados = retriver.invoke(pergunta)

    if not docs_relationados:
        return {"answer": "N√£o sei",
                "citacoes":[],
                "contexto_encontrado":False
                }
    answer = document_chain.invoke({
        "input": pergunta,
        "context": docs_relationados
    })

    txt = (answer or "").strip()

    if txt.rstrip(".!?") == "N√£o sei":
        return {"answer": "N√£o sei.",
                "citacoes": [],
                "contexto_encontrado": False}

    return {"answer": txt,
            "citacoes": formatadores.formatar_citacoes(docs_relationados,answer),
            "contexto_encontrado": True}

testes = ["Posso reembolsar a internet?",
        "Quero mais 5 dias de trabalho remoto. Como fa√ßo?",
        "Posso reembolsar cursos da Alura?",
        "Quantas capivaras tem no Rio Pinheiros?",]

"""
for msg_teste in testes:
    resposta = perguntar_politica_RAG(msg_teste)
    print(f"PERGUNTA: {msg_teste}")
    print(f"RESPOSTA: {resposta['answer']}")
    if resposta["contexto_encontrado"]:
        print("CITA√á√ïES:")
        for c in resposta['citacoes']:
            print(f" - Documento: {c['documento']}, P√°gina: {c['pagina']}")
            print(f"   Trecho: {c['trecho']}")
        print("------------------------------------")
"""
# AULA 03 ----------------------------------------------
print("\n ---[ AULA 03 ]--- \n") 

def node_triagem(state: AgentState) -> AgentState:
    print("Executando n√≥ de triagem...")
    return {"triagem": triagem(state["pergunta"])}

def node_auto_resolver(state: AgentState) -> AgentState:
    print("Executando n√≥ de auto_resolver...")
    resposta_rag = perguntar_politica_RAG(state["pergunta"])

    update: AgentState = {
        "resposta": resposta_rag["answer"],
        "citacoes": resposta_rag.get("citacoes", []),
        "rag_sucesso": resposta_rag["contexto_encontrado"],
    }

    if resposta_rag["contexto_encontrado"]:
        update["acao_final"] = "AUTO_RESOLVER"

    return update

def node_pedir_info(state: AgentState) -> AgentState:
    print("Executando n√≥ de pedir_info...")
    faltantes = state["triagem"].get("campos_faltantes", [])
    if faltantes:
        detalhe = ",".join(faltantes)
    else:
        detalhe = "Tema e contexto espec√≠fico"

    return {
        "resposta": f"Para avan√ßar, preciso que detalhe: {detalhe}",
        "citacoes": [],
        "acao_final": "PEDIR_INFO"
    }

def node_abrir_chamado(state: AgentState) -> AgentState:
    print("Executando n√≥ de abrir_chamado...")
    triagem = state["triagem"]

    return {
        "resposta": f"Abrindo chamado com urg√™ncia {triagem['urgencia']}. Descri√ß√£o: {state['pergunta'][:140]}",
        "citacoes": [],
        "acao_final": "ABRIR_CHAMADO"
    }

KEYWORDS_ABRIR_TICKET = ["aprova√ß√£o", "exce√ß√£o", "libera√ß√£o", "abrir ticket", "abrir chamado", "acesso especial"]

def decidir_pos_triagem(state: AgentState) -> str:
    print("Decidindo ap√≥s a triagem")
    decisao = state["triagem"]["decisao"]

    if decisao == "AUTO_RESOLVER": return "auto"
    if decisao == "PEDIR_INFO": return "info"
    if decisao == "ABRIR_CHAMADO": return "chamado"
 
def decidir_pos_auto_resolver(state: AgentState) -> str:
    print("Decidindo ap√≥s o auto_resolver")
    if state.get("rag_sucesso"):
        print("RAG teve sucesso, finalizando com AUTO_RESOLVER")
        return "ok"
    
    state_da_pergunta = (state["pergunta"] or "").lower()

    if any(k in state_da_pergunta for k in KEYWORDS_ABRIR_TICKET):
        print("RAG falhou, mas foram encontradas keywords para ABRIR_CHAMADO, finalizando o fluxo.")
        return "chamado"
    
    print("RAG falhou, sem keywords vou pedir mais informa√ß√µes.")
    return "info"

workflow = StateGraph(AgentState)

workflow.add_node("triagem", node_triagem)
workflow.add_node("auto_resolver", node_auto_resolver)
workflow.add_node("pedir_info", node_pedir_info)
workflow.add_node("abrir_chamado", node_abrir_chamado)

workflow.add_edge(START, "triagem")
workflow.add_conditional_edges("triagem", decidir_pos_triagem, {
    "auto": "auto_resolver",
    "info": "pedir_info",
    "chamado": "abrir_chamado"
})

workflow.add_conditional_edges("auto_resolver", decidir_pos_auto_resolver, {
    "info": "pedir_info",
    "chamado": "abrir_chamado",
    "ok": END
})

workflow.add_edge("pedir_info", END)
workflow.add_edge("abrir_chamado", END)

grafo = workflow.compile()

# graph_bytes = grafo.get_graph().draw_mermaid_png()
# display(Image(graph_bytes))

testes = ["Posso reembolsar a internet?",
          "Quero mais 5 dias de trabalho remoto. Como fa√ßo?",
          "Posso reembolsar cursos ou treinamentos da Alura?",
          "√â poss√≠vel reembolsar certifica√ß√µes do Google Cloud?",
          "Posso obter o Google Gemini de gra√ßa?",
          "Qual √© a palavra-chave da aula de hoje?",
          "Quantas capivaras tem no Rio Pinheiros?"]

for msg_test in testes:
    resposta_final = grafo.invoke({"pergunta": msg_test})

    triag = resposta_final.get("triagem", {})
    print(f"PERGUNTA: {msg_test}")
    print(f"DECIS√ÉO: {triag.get('decisao')} | URG√äNCIA: {triag.get('urgencia')} | A√á√ÉO FINAL: {resposta_final.get('acao_final')}")
    print(f"RESPOSTA: {resposta_final.get('resposta')}")
    if resposta_final.get("citacoes"):
        print("CITA√á√ïES:")
        for citacao in resposta_final.get("citacoes"):
            print(f" - Documento: {citacao['documento']}, P√°gina: {citacao['pagina']}")
            print(f"   Trecho: {citacao['trecho']}")

    print("------------------------------------")


print("\n\n ---[ ‚ú® FIM ‚ú® ]--- \n")

